---
layout: post
title: "Linux系统资源分析之 - IO"
author: Ahao Mu
tags:
- Linux
---

## 概述
__基本概念__

* 读写IO(Read/Write IO)操作
* 单个IO操作
* 随机访问(Random Access)与连续访问(Sequential Access)
* 顺序IO模式(Queue Mode)/并发IO模式(Burst Mode)
* 单个IO的大小(IO Chunk Size)

__IO相关性能指标__

* IOPS (Input/Output Per Second)
	* 即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。IOPS是指单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求数量为单位，I/O请求通常为读或写数据操作请求。随机读写频繁的应用，如OLTP(Online Transaction Processing)，IOPS是关键衡量指标。
* 数据吞吐量(Throughput)
	* 指单位时间内可以成功传输的数据数量。对于大量顺序读写的应用，如电视台的视频编辑，视频点播VOD(Video On Demand)，则更关注吞吐量指标。
* IO响应时间(IO Response Time)
	* IO响应时间也被称为IO延时(IO Latency)，IO响应时间就是从操作系统内核发出的一个读或者写的IO命令到操作系统内核接收到IO回应的时间，注意不要和单个IO时间混淆了，单个IO时间仅仅指的是IO操作在磁盘内部处理的时间，而IO响应时间还要包括IO操作在IO等待队列中所花费的等待时间。 

## 几个基本的概念
　　在研究磁盘性能之前我们必须先了解磁盘的结构，以及工作原理。不过在这里就不再重复说明了，关系硬盘结构和工作原理的信息可以参考维基百科上面的相关词条——Hard disk drive(英文)和硬盘驱动器(中文)。
### 读写IO(Read/Write IO)操作
　　磁盘是用来给我们存取数据用的，因此当说到IO操作的时候，就会存在两种相对应的操作，存数据时候对应的是写IO操作，取数据的时候对应的是是读IO操作。
### 单个IO操作
　　当控制磁盘的控制器接到操作系统的读IO操作指令的时候，控制器就会给磁盘发出一个读数据的指令，并同时将要读取的数据块的地址传递给磁盘，然后磁盘会将读取到的数据传给控制器，并由控制器返回给操作系统，完成一个写IO的操作;同样的，一个写IO的操作也类似，控制器接到写的IO操作的指令和要写入的数据，并将其传递给磁盘，磁盘在数据写入完成之后将操作结果传递回控制器，再由控制器返回给操作系统，完成一个写IO的操作。单个IO操作指的就是完成一个写IO或者是读IO的操作。
### 随机访问(Random Access)与连续访问(Sequential Access)
　　随机访问指的是本次IO所给出的扇区地址和上次IO给出扇区地址相差比较大，这样的话磁头在两次IO操作之间需要作比较大的移动动作才能重新开始读/写数据。相反的，如果当次IO给出的扇区地址与上次IO结束的扇区地址一致或者是接近的话，那磁头就能很快的开始这次IO操作，这样的多个IO操作称为连续访问。因此尽管相邻的两次IO操作在同一时刻发出，但如果它们的请求的扇区地址相差很大的话也只能称为随机访问，而非连续访问。
### 顺序IO模式(Queue Mode)/并发IO模式(Burst Mode)
　　磁盘控制器可能会一次对磁盘组发出一连串的IO命令，如果磁盘组一次只能执行一个IO命令时称为顺序IO;当磁盘组能同时执行多个IO命令时，称为并发IO。并发IO只能发生在由多个磁盘组成的磁盘组上，单块磁盘只能一次处理一个IO命令。

### 单个IO的大小(IO Chunk Size)
熟悉数据库的人都会有这么一个概念，那就是数据库存储有个基本的块大小(Block Size)，不管是SQL Server还是Oracle，默认的块大小都是8KB，就是数据库每次读写都是以8k为单位的。那么对于数据库应用发出的固定8k大小的单次读写到了写磁盘这个层面会是怎么样的呢，就是对于读写磁盘来说单个IO操作操作数据的大小是多少呢，是不是也是一个固定的值?

答案是不确定。首先操作系统为了提高 IO的性能而引入了文件系统缓存(File System Cache)，系统会根据请求数据的情况将多个来自IO的请求先放在缓存里面，然后再一次性的提交给磁盘，也就是说对于数据库发出的多个8K数据块的读操作有可能放在一个磁盘读IO里就处理了。

还有对于有些存储系统也是提供了缓存(Cache)的，接收到操作系统的IO请求之后也是会将多个操作系统的 IO请求合并成一个来处理。不管是操作系统层面的缓存还是磁盘控制器层面的缓存，目的都只有一个，提高数据读写的效率。因此每次单独的IO操作大小都是不一样的，它主要取决于系统对于数据读写效率的判断。

当一次IO操作大小比较小的时候我们成为小的IO操作，比如说1K，4K，8K这样的;当一次IO操作的数据量比较的的时候称为大IO操作，比如说32K，64K甚至更大。

在我们说到块大小(Block Size)的时候通常我们会接触到多个类似的概念，像我们上面提到的那个在数据库里面的数据最小的管理单位，Oralce称之为块(Block)，大小一般为8K，SQL Server称之为页(Page)，一般大小也为8k。

在文件系统里面我们也能碰到一个文件系统的块，在现在很多的Linux系统中都是4K(通过```/usr/bin/time -v```可以看到)，它的作用其实跟数据库里面的块/页是一样的，都是为了方便数据的管理。但是说到单次IO的大小，跟这些块的大小都是没有直接关系的，在英文里单次IO大小通常被称为是IO Chunk Size，不会说成是IO Block Size的。
## IO相关的性能指标
* IOPS (Input/Output Per Second)
	* 即每秒的输入输出量(或读写次数)，是衡量磁盘性能的主要指标之一。IOPS是指单位时间内系统能处理的I/O请求数量，一般以每秒处理的I/O请求数量为单位，I/O请求通常为读或写数据操作请求。随机读写频繁的应用，如OLTP(Online Transaction Processing)，IOPS是关键衡量指标。
* 数据吞吐量(Throughput)
	* 指单位时间内可以成功传输的数据数量。对于大量顺序读写的应用，如电视台的视频编辑，视频点播VOD(Video On Demand)，则更关注吞吐量指标。
* IO响应时间(IO Response Time)
	* IO响应时间也被称为IO延时(IO Latency)，IO响应时间就是从操作系统内核发出的一个读或者写的IO命令到操作系统内核接收到IO回应的时间，注意不要和单个IO时间混淆了，单个IO时间仅仅指的是IO操作在磁盘内部处理的时间，而IO响应时间还要包括IO操作在IO等待队列中所花费的等待时间。 


### 指标1: IOPS(IO per Second)
IOPS，IO系统每秒所执行IO操作的次数，是一个重要的用来衡量系统IO能力的一个参数。对于单个磁盘组成的IO系统来说，计算它的IOPS不是一件很难的事情，只要我们知道了系统完成一次IO所需要的时间的话我们就能推算出系统IOPS来。

现在我们就来推算一下磁盘的IOPS，假设磁盘的转速(Rotational Speed)为15K RPM，平均寻道时间为5ms，最大传输速率为40MB/s(这里将读写速度视为一样，实际会差别比较大)。

对于磁盘来说一个完整的IO操作是这样进行的：当控制器对磁盘发出一个IO操作命令的时候，磁盘的驱动臂(Actuator Arm)带读写磁头(Head)离开着陆区(Landing Zone，位于内圈没有数据的区域)，移动到要操作的初始数据块所在的磁道(Track)的正上方，这个过程被称为寻址(Seeking)，对应消耗的时间被称为寻址时间(Seek Time);但是找到对应磁道还不能马上读取数据，这时候磁头要等到磁盘盘片(Platter)旋转到初始数据块所在的扇区(Sector)落在读写磁头正上方的之后才能开始读取数据，在这个等待盘片旋转到可操作扇区的过程中消耗的时间称为旋转延时(Rotational Delay);接下来就随着盘片的旋转，磁头不断的读/写相应的数据块，直到完成这次IO所需要操作的全部数据，这个过程称为数据传送(Data Transfer)，对应的时间称为传送时间(Transfer Time)。完成这三个步骤之后一次IO操作也就完成了。

在我们看硬盘厂商的宣传单的时候我们经常能看到3个参数，分别是平均寻址时间、盘片旋转速度以及最大传送速度，这三个参数就可以提供给我们计算上述三个步骤的时间。

* 第一个寻址时间，考虑到被读写的数据可能在磁盘的任意一个磁道，既有可能在磁盘的最内圈(寻址时间最短)，也可能在磁盘的最外圈(寻址时间最长)，所以在计算中我们只考虑平均寻址时间，也就是磁盘参数中标明的那个平均寻址时间，这里就采用当前最多的10krmp硬盘的5ms。

* 第二个旋转延时，和寻址一样，当磁头定位到磁道之后有可能正好在要读写扇区之上，这时候是不需要额外额延时就可以立刻读写到数据，但是最坏的情况确实要磁盘旋转整整一圈之后磁头才能读取到数据，所以这里我们也考虑的是平均旋转延时，对于15krpm的磁盘就```(60s/15k)*(1/2) = 2ms。``` 即```60000ms/15000/2=2ms```

* 第三个传送时间，磁盘参数提供我们的最大的传输速度，当然要达到这种速度是很有难度的，但是这个速度却是磁盘纯读写磁盘的速度，因此只要给定了单次IO的大小，我们就知道磁盘需要花费多少时间在数据传送上，这个时间就是```IO Chunk Size / Max Transfer Rate```。

#### IOPS计算方法
传统磁盘本质上一种机械装置，如FC, SAS, SATA磁盘，转速通常为5400/7200/10K/15K rpm不等。**影响磁盘的关键因素是磁盘服务时间，即磁盘完成一个I/O请求所花费的时间，它由寻道时间、旋转延迟和数据传输时间三部分构成**。

* 平均寻道时间时间
	* Tseek是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3－15ms。

* 平均旋转延迟时间
	* Trotation是指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。比如，7200 rpm的磁盘平均旋转延迟大约为```60*1000/7200/2 = 4.17ms```，而转速为15000 rpm的磁盘其平均旋转延迟约为2ms。这里为啥除以二，那是因为要的是平均旋转延迟时间平均平均！

* 数据传输时间(几近忽略)
	* **数据传输时间通常远小于前两部分时间。**
	* Ttransfer是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，**数据传输时间通常远小于前两部分时间。**

因此，理论上可以计算出磁盘的最大IOPS，即```IOPS = 1000 ms/ (Tseek + Troatation)，忽略数据传输时间```。假设磁盘平均物理寻道时间为3ms, 磁盘转速为7200,10K,15K rpm，则磁盘IOPS理论最大值分别为，

```
 IOPS = 1000ms / (3ms + 60000ms/7200/2)  = 140   #一分钟7200转，意味着，平均旋转延迟时间是:60000ms/7200/2
 IOPS = 1000ms / (3ms + 60000ms/10000/2) = 167
 IOPS = 1000ms / (3ms + 60000ms/15000/2) = 200
```

固态硬盘SSD是一种电子装置， 避免了传统磁盘在寻道和旋转上的时间花费，存储单元寻址开销大大降低，因此IOPS可以非常高，能够达到数万甚至数十万。实际测量中，IOPS数值会受到很多因素的影响，包括I/O负载特征(读写比例，顺序和随机，工作线程数，队列深度，数据记录大小)、系统配置、操作系统、磁盘驱动等等。因此对比测量磁盘IOPS时，必须在同样的测试基准下进行，即便如何也会产生一定的随机不确定性。通常情况下，IOPS可细分为如下几个指标：

#### 单场景IOPS指标
Toatal IOPS，混合读写和顺序随机I/O负载情况下的磁盘IOPS，这个与实际I/O情况最为相符，大多数应用关注此指标。

```
Random Read IOPS，100%随机读负载情况下的IOPS。
Random Write IOPS，100%随机写负载情况下的IOPS。
Sequential Read IOPS，100%顺序负载读情况下的IOPS。
Sequential Write IOPS，100%顺序写负载情况下的IOPS。
```

IOPS的测试benchmark工具主要有Iometer, IoZone, FIO等，可以综合用于测试磁盘在不同情形下的IOPS。对于应用系统，需要首先确定数据的负载特征，然后选择合理的IOPS指标进行测量和对比分析，据此选择合适的存储介质和软件系统。

### 指标2: 传输速度(Transfer Rate)/吞吐率(Throughput)
现在我们要说的传输速度(另一个常见的说法是吞吐率)不是磁盘上所表明的最大传输速度或者说理想传输速度，而是磁盘在实际使用的时候从磁盘系统总线上流过的数据量。有了IOPS数据之后我们是很容易就能计算出对应的传输速度来的

```
	Transfer Rate = IOPS * IO Chunk Size
```
```
　　还是那上面的第一组IOPS的数据我们可以得出相应的传输速度如下
　　4K: 140 * 4K = 560K / 40M = 1.36%
　　8K: 139 * 8K = 1112K / 40M = 2.71%
　　16K: 135 * 16K = 2160K / 40M = 5.27%
　　32K: 116 * 32K = 3712K / 40M = 9.06%
　　可以看出实际上的传输速度是很小的，对总线的利用率也是非常的小。
```

　　这里一定要明确一个概念，那就是尽管上面我们使用IOPS来计算传输速度，但是实际上传输速度和IOPS是没有直接关系，在没有缓存的情况下它们共同的决定因素都是对磁盘系统的访问方式以及单个IO的大小。对磁盘进行随机访问时候我们可以利用IOPS来衡量一个磁盘系统的性能，此时的传输速度不会太大;但是当对磁盘进行连续访问时，此时的IOPS已经没有了参考的价值，这个时候限制实际传输速度却是磁盘的最大传输速度。因此在实际的应用当中，只会用IOPS来衡量小IO的随机读写的性能，而当要衡量大IO连续读写的性能的时候就要采用传输速度而不能是IOPS了。

###  RAID计算磁盘IOPS的三个因素
上面说的都是单块磁盘的IOPS的计算方法，和理解，在实际生产中，我们使用RAID，这就会大大提高整体的IOPS.但是，不同的raid类型，IOPS的计算方法是不一样的，不是和磁盘的数量1：1 的关系简单的累加！！

计算IOPS还和读，写比有关系

#### 1、RAID类型的读写比

不同RAID类型的IOPS计算公式：

RAID类型|公式
------|-----
RAID5、RAID3|Drive IOPS=Read IOPS + 4*Write IOPS
RAID6|DriveIOPS=Read IOPS +  6*Write IOPS
RAID1、RAID10|Drive IOPS=Read IOPS + 2*Write IOPS

#### 2、硬盘类型的IOPS值 

不同磁盘类型的IOPS

硬盘类型|IOPS
-----|-----
FC 15K RPM|180
FC 10K RPM|140
SAS 15K RPM|180
SAS 10K RPM|150
SATA 10K RPM|290
SATA 7.2K RPM|80
SATA 5.4K RPM|40
Flash drive|2500

#### 3、具体业务系统的读写比
##### 案例
1) 业务需求： 10TB 的FC 15K RPM存储空间，满足6000 IOPS，计算RAID5，RAID10分别需要多少块硬盘？
首先需要知道I/O中读操作与写操作所占的百分比。 假定6000 IOPS中读/写比是2：1
不同的RAID类型Drive 硬盘实际IOPS负载分别如下：

```
RAID10：（2/3）*6000+2*（1/3）*6000= 8000 IOPS
RAID5：（2/3）*6000+4*（1/3）*6000=12000 IOPS
```
参照不同硬盘类型的IOPS值，换算出需要多少块盘：

```
RAID10：8000 /180 = 45块
RAID5：12000/180 =67块
```
2) 一个RAID5，是由5块500G 10K RPM的FC盘组成，换算出该RAID支持的最大IOPS以及能够给前端应用提供的IOPS？

首先10K RPM的FC盘，单块盘的IOPS为140，5块盘最大IOPS值为700。

假设读写比为2：1，能够提供给应用的IOPS为：

```
（2/3）*X+4*（1/3）*X = 700
      2*X = 700 
      X=350
```

#### 如何查看raid信息？
https://raid.wiki.kernel.org/index.php/Mdstat

The /proc/mdstat file shows a snapshot of the kernel's RAID/md state.

```
[root@localhost-1 /home/ahao.mah]
#cat /proc/mdstat
Personalities : [raid6] [raid5] [raid4]
md0 : active raid5 sdl[10](S) sdj[8] sdf[4] sdd[2] sdh[6] sde[3] sdb[0] sdc[1] sdg[5] sdi[7] sdk[11]
      17580450816 blocks super 1.2 level 5, 512k chunk, algorithm 2 [10/10] [UUUUUUUUUU]
      bitmap: 0/15 pages [0KB], 65536KB chunk

unused devices: <none>
```
* 第一行： Personalities : [raid6] [raid5] [raid4] 代表kernel支持的raid类型，如果想要更改需要重新编译内核
* 第二行：md0 表示看的就是/dev/md0这个device, It is active or 'started'. An inactive array is usually faulty. Stopped arrays aren't visible here.
* 第三行：md配置状态栏，当前可用块：17580450816 blocks， the array uses a 1.2 superblock ，and confirms a level 5 (this is redundant!) array with a chunk size of 64k using algorithm 2 ，这个[10/10] 可以看成[n/m],意味着理想情况下，阵列将具有n个设备，但目前，m个设备正在使用中，显然，当m> = n时，是最好的。[UUUUUUUUUU]表示每个设备的状态，U为up或_为down。

### 指标3: IO响应时间(IO Response Time)
最后来关注一下能直接描述IO性能的IO响应时间。IO响应时间也被称为IO延时(IO Latency)，**IO响应时间就是从操作系统内核发出的一个读或者写的IO命令到操作系统内核接收到IO回应的时间，注意不要和单个IO时间混淆了，单个IO时间仅仅指的是IO操作在磁盘内部处理的时间，而IO响应时间还要包括IO操作在IO等待队列中所花费的等待时间。**

计算IO操作在等待队列里面消耗的时间有一个衍生于利托氏定理(Little’s Law)的排队模型M/M/1模型可以遵循，由于排队模型算法比较复杂，到现在还没有搞太明白(如果有谁对M/M/1模型比较精通的话欢迎给予指导)，这里就罗列一下最后的结果，还是那上面计算的IOPS数据来说：

```
　　8K IO Chunk Size (135 IOPS, 7.2 ms)
　　135 => 240.0 ms
　　105 => 29.5 ms
　　75 => 15.7 ms
　　45 => 10.6 ms
　　64K IO Chunk Size(116 IOPS, 8.6 ms)
　　135 => 没响应了……
　　105 => 88.6 ms
　　75 => 24.6 ms
　　45 => 14.6 ms
```

从上面的数据可以看出，随着系统实际IOPS越接近理论的最大值，IO的响应时间会成非线性的增长，越是接近最大值，响应时间就变得越大，而且会比预期超出很多。**一般来说在实际的应用中有一个70%的指导值，也就是说在IO读写的队列中，当队列大小小于最大IOPS的70%的时候，IO的响应时间增加会很小，相对来说让人比较能接受的，一旦超过70%，响应时间就会戏剧性的暴增，所以当一个系统的IO压力超出最大可承受压力的70%的时候就是必须要考虑调整或升级了。**

另外补充说一下这个70%的指导值也适用于CPU响应时间，这也是在实践中证明过的，一旦CPU超过70%，系统将会变得受不了的慢。很有意思的东西。

### 机械硬盘的特性
同一方向的操作是合并起来完成的，而后在这个方向结束之后则是另外一方向的
对硬盘来讲，读写是不同类型的操作，读写是不能同时进行的
 
### 磁盘是如何操作的
将一个或多个进程的读操作合并到一起读
将一个或多个进程的写操作合并到一起写
 
所以读写操作是两类不同的操作而且是同一方向合并的
如果是读文件，这个文件一定是来自于磁盘的
如果是写文件，那么写入到内存中，对于进程来讲是已经完成的，那么用户对计算机性能感知是来自于读，因为读一定是与IO相交互
 
1. 读是在同方向合并的
2. 写也是需要合并的，而且两者是不同方向的操作

因为在同一方向可以节省很多资源
读必须优先满足，而写也不能等太久，因此必须有一种良好的算法让其尽可能都得到满足，而又不能让用户感到性能下降  

### IO调度器
IO调度器主要功能就是将随机IO尽可能合并为顺序IO,但是我们有说过，尽可能同一方向合并,尽可能会随机变为顺序，但是我们又不得不读饥饿也不能写饥饿，所以要交替进行的

### 调度算法
IO调度器事实上是用程序完成的调度算法，对linux来讲，2.6的内核一共有4个

* CFQ
	* 完全公平队列，比较适合于交互式场景      
* Deadline
   * 最后期限，任何一个读写请求,都有自己的满足期限，当期限到来时之前，必须达到需求的满足（一般建议在数据库服务器上使用此调度算法）
* anticpatory
   * 预期的,任何一个数据读完之后，有可能与其相邻的数据也可能被读到，所以它大致所实现的方法就是，读完之后先不满足，则不处理，需等一段时间后查看是否有相近数据访问过，如果有马上先满足，所以这只能在行为预估的场景下可用
* Noop
    * 不排队不合并，先到先得

像固态硬盘，因为它不是机械硬盘，它的读写就算是随机IO那么它的性能跟顺序IO差别也不是很大，反而如果想让调度器去调取它的算法，那么调度器本身运行会占用很高的CPU的时钟周期，有可能会得不偿失，所以noop在这种场景下是最好的算法

有些RAID设备控制器在硬件设备上自己就有读写操作排序的，也就意味着在硬件级别排好序之后在操作系统级别会将其打散重新排序，得不偿失，所以RAID设备有自己的调度器的话，最好也使用noop

一般来讲，默认是CFQ

### 调整buffer，提高性能
调整队列数，以及增加预读数

* 增加队列长度

```
[root@localhost-1 /home/ahao.mah]
#cat /sys/block/sda/queue/nr_requests
128

```

* 增加预读数

```
[root@localhost-1 /home/ahao.mah]
#cat /sys/block/sda/queue/read_ahead_kb
128
```

## IO监控
### 进程级IO监控
#### iotop 
iotop的本质是一个python脚本，从proc中获取thread的IO信息，进行汇总。

这两个命令,都可以按进程统计IO状况,因此可以回答你以下二个问题

1. 当前系统哪些进程在占用IO,百分比是多少?
2. 占用IO的进程是在读?还是在写?读写量是多少?

```
[root@localhost-1 /var/log/journal]
#iotop
Total DISK READ :      14.92 K/s | Total DISK WRITE :     276.12 M/s
Actual DISK READ:      14.92 K/s | Actual DISK WRITE:       0.00 B/s
   TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO>    COMMAND
  4438 be/4 root        7.46 K/s    3.73 K/s  0.00 %  0.94 % DragoonAgent
 22210 be/4 root        7.46 K/s  276.11 M/s  0.00 %  0.03 % dd if=/dev/zero of=./bigfile bs=1K count=100000000
     1 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % systemd --switched-root --system --deserialize 21
     2 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kthreadd]
     3 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [ksoftirqd/0]
     5 be/0 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [kworker/0:0H]
     7 rt/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [migration/0]
     8 be/4 root        0.00 B/s    0.00 B/s  0.00 %  0.00 % [rcu_bh]
```
```
[root@localhost-1 /data]
#dd if=/dev/zero of=./bigfile bs=1K count=100000000
^C2329821+0 records in
2329821+0 records out
2385736704 bytes (2.4 GB) copied, 9.17054 s, 260 MB/s
```
#### pidstat

```
[root@localhost-1 /var/log/journal]
#pidstat -d  1
10:30:15 AM   UID       PID   kB_rd/s   kB_wr/s kB_ccwr/s  Command
10:30:16 AM     0     22847     12.00 283240.00      0.00  dd
10:30:16 AM     0     25240      0.00      4.00      0.00  staragent-ppf
```

```
pidstat -u -r -d -t 1        
# -d IO 信息,
# -r 缺页及内存信息
# -u CPU使用率
# -t 以线程为统计单位
# 1  1秒统计一次
```

### block_dump, iodump
iotop和 pidstat 用着很爽,但两者都依赖于```/proc/pid/io```文件导出的统计信息, 这个对于老一些的内核是没有的,比如rhel5u2

```
[root@localhost-1 /var/log/journal]
#cat /proc/24092/io
rchar: 319688167
wchar: 433
syscr: 1836114
syscw: 12
read_bytes: 0
write_bytes: 28479488
cancelled_write_bytes: 0
```
因此只好用以上2个穷人版命令来替代:

```
echo 1 > /proc/sys/vm/block_dump     # 开启block_dump,此时会把io信息输入到dmesg中
watch -n 1 "dmesg -c | grep -oP \"\w+\(\d+\): (WRITE|READ)\" | sort | uniq -c"
echo 0 > /proc/sys/vm/block_dump      # 不用时关闭
```

### 业务级IO监控
#### pt-ioprofile
安装

```
yum install -y percona-toolkit -b current
```

[https://www.percona.com/downloads/percona-toolkit/2.2.1/](https://www.percona.com/downloads/percona-toolkit/2.2.1/)

pt-ioprofile的原理是对某个pid附加一个strace进程进行IO分析,ioprofile 命令本质上是 lsof + strace.

ioprofile 可以回答你以下三个问题:

1. 当前进程某时间内,在业务层面读写了哪些文件(read, write)？
2. 读写次数是多少?(read, write的调用次数)
3. 读写数据量多少?(read, write的byte数)

假设某个行为会触发程序一次IO动作,例如: "一个页面点击,导致后台读取A,B,C文件"

./io_event   # 假设模拟一次IO行为,读取A文件一次, B文件500次, C文件500次

```
ioprofile  -p  `pidof  io_event` -c count   # 读写次数
ioprofile  -p  `pidof  io_event` -c times   # 读写耗时
ioprofile  -p  `pidof  io_event` -c sizes    # 读写大小
```

### 系统级IO监控
#### iostat

```
[root@localhost-1 /data]
#dd if=/dev/zero of=./bigfile bs=1K count=100000000
^C8588301+0 records in
8588301+0 records out
8794420224 bytes (8.8 GB) copied, 30.9121 s, 284 MB/s
```

```
[root@localhost-1 /home/ahao.mah]
#iostat -dxm 1
Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
sdb             271.00  6749.00  170.00  901.00     1.75    30.04    60.79     7.49    6.89    9.40    6.42   0.46  49.00
sda               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
sdk             253.00  6749.00  178.00  903.00     1.70    30.05    60.16    12.65   11.61   10.19   11.89   0.67  72.00
sdj             264.00  6737.00  174.00  915.00     1.73    29.92    59.52    11.11    9.99   10.03    9.98   0.57  62.00
sde             406.00  6862.00  185.00  899.00     2.32    30.46    61.93    11.02   10.12    7.45   10.67   0.55  59.60
sdl               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00
sdg             284.00  6731.00  180.00  904.00     1.74    29.77    59.52    10.03    9.06    8.00    9.27   0.53  57.30
sdd             366.00  6749.00  179.00  887.00     2.15    29.79    61.36    11.93   11.05   12.45   10.77   0.60  64.00
sdi             143.00  6757.00  174.00  891.00     1.18    29.87    59.70    12.27   11.26   17.69   10.01   0.62  66.20
sdc             448.00  6893.00  167.00  850.00     2.29    30.36    65.75    18.87   16.65   30.11   14.00   0.79  80.40
sdf             256.00  6773.00  169.00  838.00     1.67    29.77    63.94    12.69   12.33   16.56   11.47   0.64  64.60
sdh             169.00  6790.00  188.00  843.00     1.33    29.96    62.16    18.25   17.36   26.18   15.39   0.74  76.70
md0               0.00     0.00    3.00 46195.00     0.01   269.77    11.96     0.00    0.00    0.00    0.00   0.00   0.00
```
* %util
	* 代表磁盘繁忙程度。100% 表示磁盘繁忙, 0%表示磁盘空闲。但是注意,磁盘繁忙不代表磁盘(带宽)利用率高  
* argrq-sz    
	* 提交给驱动层的IO请求大小,一般不小于4K,不大于max(readahead_kb, max_sectors_kb),可用于判断当前的IO模式,一般情况下,尤其是磁盘繁忙时, 越大代表顺序,越小代表随机
* svctm        
	* 一次IO请求的服务时间,对于单块盘,完全随机读时,基本在7ms左右,既寻道+旋转延迟时间

注: 各统计量之间关系

```
%util = ( r/s  +  w/s) * svctm / 1000                        # 队列长度 =  到达率     *  平均服务时间                     
avgrq-sz = ( rMB/s + wMB/s) * 2048 / (r/s  + w/s)    # 2048 为 1M / 512
```


#### 总结:
iostat 统计的是通用块层经过合并(rrqm/s, wrqm/s)后,直接向设备提交的IO数据,可以反映系统整体的IO状况,但是有以下2个缺点:

1. 距离业务层比较遥远,跟代码中的write,read不对应(由于系统预读 + pagecache + IO调度算法等因素, 也很难对应)
2. 是系统级,没办法精确到进程,比如只能告诉你现在磁盘很忙,但是没办法告诉你是谁在忙,在忙什么？

